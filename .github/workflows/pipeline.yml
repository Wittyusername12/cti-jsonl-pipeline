name: metrics-id-final

on:
  workflow_dispatch: {}
  push:
    branches: [ main ]

jobs:
  id-final:
    runs-on: windows-latest
    timeout-minutes: 20

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      # 1) Fail-fast canonicalization (ID mode)
      - name: Preflight (ID mode)
        shell: pwsh
        run: |
          py preflight.py --mode id `
            --top1 ce_top1_final_ID.csv `
            --gold gold_id_unbalanced_nobom.jsonl `
            --out-csv top1_eval.csv `
            --out-gold gold_eval.jsonl `
            --overlap-floor 10

      # 2) Deterministic micro sample (first 100 overlapped) to avoid flakiness
      - name: Make deterministic micro sample (first 100 overlaps)
        shell: pwsh
        run: |
          python - << 'PY'
          import csv, json
          from pathlib import Path
          # load overlapped queries from canonical gold
          qset=set()
          with open('gold_eval.jsonl', encoding='utf-8') as f:
              for line in f:
                  if line.strip():
                      o=json.loads(line); q=o.get('query','').strip()
                      if q: qset.add(q)
          # take first 100 overlapped rows deterministically
          rows=[]
          with open('top1_eval.csv', newline='', encoding='utf-8') as f:
              r=csv.DictReader(f)
              for row in r:
                  if row.get('query','') in qset:
                      rows.append({'query':row['query'], 'candidate':row['candidate']})
          rows = rows[:100]
          with open('sample50_eval.csv','w',newline='',encoding='utf-8') as f:
              w=csv.DictWriter(f, fieldnames=['query','candidate'])
              w.writeheader(); w.writerows(rows)
          print(f"Wrote sample50_eval.csv with {len(rows)} rows")
          PY

      # 3) Micro-eval gate (relaxed to 0.08 to account for sample size)
      - name: Micro-eval
        shell: pwsh
        run: |
          py metrics_top1.py --top1_csv sample50_eval.csv `
                             --gold_jsonl gold_eval.jsonl `
                             --out micro_metrics.txt
          Get-Content micro_metrics.txt

      - name: Assert micro P@1 >= 0.08
        shell: pwsh
        run: |
          $t = Get-Content micro_metrics.txt -Raw
          $m = [regex]::Match($t, 'P@1 \(overall\):\s*([0-9.]+)')
          if (-not $m.Success) { throw "Could not parse P@1 from micro metrics." }
          $p = [double]$m.Groups[1].Value
          "Micro P@1=$p"
          if ($p -lt 0.08) { throw "Micro P@1 $p < 0.08; aborting full run." }

      # 4) Full eval on canonical files
      - name: Full metrics (ID)
        shell: pwsh
        run: |
          py metrics_top1.py --top1_csv top1_eval.csv `
                             --gold_jsonl gold_eval.jsonl `
                             --out metrics_unbalanced_p1_id_final.txt
          Get-Content metrics_unbalanced_p1_id_final.txt

      # 5) Misses report (top 40)
      - name: Misses Top 40
        shell: pwsh
        run: |
          py misses_report.py --top1_csv top1_eval.csv `
                              --gold_jsonl gold_eval.jsonl `
                              --out_csv debug_misses_top40.csv `
                              --k 40 `
                              --p_col p_relevant `
                              --pirr_col p_irrelevant

      # 6) Assert final floor so green != junk
      - name: Assert P@1 >= 0.10
        shell: pwsh
        run: |
          $txt = Get-Content metrics_unbalanced_p1_id_final.txt -Raw
          $m = [regex]::Match($txt, 'P@1 \(overall\):\s*([0-9.]+)')
          if (-not $m.Success) { throw "Could not parse P@1 from metrics." }
          $p1 = [double]$m.Groups[1].Value
          "Parsed P@1 = $p1"
          if ($p1 -lt 0.10) { throw "P@1 $p1 < 0.10 threshold" }

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: id-final-artifacts
          path: |
            manifest.json
            sample50_eval.csv
            top1_eval.csv
            gold_eval.jsonl
            micro_metrics.txt
            metrics_unbalanced_p1_id_final.txt
            debug_misses_top40.csv
